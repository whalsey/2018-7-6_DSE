{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size:(167888, 87)\n",
      "Sample Size:(2985217, 7)\n"
     ]
    }
   ],
   "source": [
    "#Importing Libraries or Packages that are needed throughout the Program \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import xgboost as  xgb \n",
    "import random \n",
    "import datetime as dt \n",
    "import gc\n",
    "\n",
    "# Load the Datasets #\n",
    "\n",
    "DirBase = \"/Volumes/wms_ssd/Qualifying Exam/\"\n",
    "\n",
    "# We need to load the datasets that will be needed to train our machine learning algorithms, handle our data and \n",
    "# make predictions. Note that these datasets are the ones that are already provided once you enter the competition\n",
    "# by accepting terms and conditions\n",
    "\n",
    "train = pd.read_csv(DirBase+'Input/data.csv')\n",
    "properties_2016_df = pd.read_csv(DirBase+'Input/properties_2016_mod.csv')\n",
    "properties_2017_df = pd.read_csv(DirBase+'Input/properties_2017_mod.csv')\n",
    "test = pd.read_csv(DirBase+'Output/sample_submission.csv') \n",
    "test = test.rename(columns={'ParcelId': 'parcelid'}) # To make it easier for merging datasets on same column_id later\n",
    "output = test.copy()\n",
    "\n",
    "# Analyse the Dimensions of our Datasets.\n",
    "\n",
    "print(\"Training Size:\" + str(train.shape))\n",
    "print(\"Sample Size:\" + str(test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage reduction...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Type Converting the DataSet \n",
    "\n",
    "# The processing of some of the algorithms can be made quick if data representation is made in int/float32 instead\n",
    "# of int/float64. Therefore, in order to make sure that all of our columns types are in float32, we are\n",
    "# implementing the following lines of code #\n",
    "\n",
    "for c, dtype in zip(train.columns, train.dtypes):\n",
    "    if dtype == np.float64:        \n",
    "        train[c] = train[c].astype(np.float32)\n",
    "    if dtype == np.int64:\n",
    "        train[c] = train[c].astype(np.int32)\n",
    "\n",
    "for column in test.columns:\n",
    "    if test[column].dtype == int:\n",
    "        test[column] = test[column].astype(np.int32)\n",
    "    if test[column].dtype == float:\n",
    "        test[column] = test[column].astype(np.float32)\n",
    "\n",
    "df_test_2016 = test.merge(properties_2016_df, how='left', on='parcelid')\n",
    "df_test_2017 = test.merge(properties_2017_df, how='left', on='parcelid')\n",
    "\n",
    "### Remove previous variables to keep some memory\n",
    "del properties_2016_df, properties_2017_df\n",
    "\n",
    "print('Memory usage reduction...')\n",
    "train[['latitude', 'longitude']] /= 1e6\n",
    "df_test_2016[['latitude', 'longitude']] /= 1e6\n",
    "df_test_2017[['latitude', 'longitude']] /= 1e6\n",
    "\n",
    "train['censustractandblock'] /= 1e12\n",
    "df_test_2016['censustractandblock'] /= 1e12\n",
    "df_test_2017['censustractandblock'] /= 1e12\n",
    "\n",
    "### Rearranging the DataSets ###\n",
    "\n",
    "# We will now drop the features that serve no useful purpose. We will also split our data and divide it into the\n",
    "# representation to make it clear which features are to be treated as determinants in predicting the outcome for\n",
    "# our target feature. Make sure to include the same features in the test set as were included in the training set\n",
    "\n",
    "train['sign'] = train.apply(lambda row: 0 if row['logerror'] < -1 else 1, axis=1)\n",
    "\n",
    "x_train = train.drop(['parcelid', 'sign', 'logerror', 'propertyzoningdesc', \n",
    "                         'propertycountylandusecode', 'month', 'year'], axis=1)\n",
    "\n",
    "df_test_2016 = df_test_2016.drop(['parcelid', 'propertyzoningdesc',\n",
    "                       'propertycountylandusecode', '201610', '201611', \n",
    "                       '201612', '201710', '201711', '201712'], axis = 1) \n",
    "\n",
    "df_test_2017 = df_test_2017.drop(['parcelid', 'propertyzoningdesc',\n",
    "                       'propertycountylandusecode', '201610', '201611', \n",
    "                       '201612', '201710', '201711', '201712'], axis = 1) \n",
    "\n",
    "x_train = x_train.values\n",
    "y_train_1 = abs(train['logerror'].values)\n",
    "y_train_2 = train['sign']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sign'] = train.apply(lambda row: 0 if row['logerror'] < 0 else 1, axis=1)\n",
    "y_train_2 = train['sign'].values\n",
    "# print(train['sign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain_1-mae:0.430384\tvalid_1-mae:0.430026\n",
      "Multiple eval metrics have been passed: 'valid_1-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid_1-mae hasn't improved in 100 rounds.\n",
      "[10]\ttrain_1-mae:0.310414\tvalid_1-mae:0.310104\n",
      "[20]\ttrain_1-mae:0.228337\tvalid_1-mae:0.227938\n",
      "[30]\ttrain_1-mae:0.172681\tvalid_1-mae:0.172298\n",
      "[40]\ttrain_1-mae:0.135277\tvalid_1-mae:0.134962\n",
      "[50]\ttrain_1-mae:0.110359\tvalid_1-mae:0.110085\n",
      "[60]\ttrain_1-mae:0.09389\tvalid_1-mae:0.093628\n",
      "[70]\ttrain_1-mae:0.083034\tvalid_1-mae:0.082778\n",
      "[80]\ttrain_1-mae:0.075919\tvalid_1-mae:0.075689\n",
      "[90]\ttrain_1-mae:0.071187\tvalid_1-mae:0.070972\n",
      "[100]\ttrain_1-mae:0.068055\tvalid_1-mae:0.067869\n",
      "[110]\ttrain_1-mae:0.065952\tvalid_1-mae:0.065797\n",
      "[120]\ttrain_1-mae:0.064521\tvalid_1-mae:0.064408\n",
      "[130]\ttrain_1-mae:0.063544\tvalid_1-mae:0.063472\n",
      "[140]\ttrain_1-mae:0.062841\tvalid_1-mae:0.062804\n",
      "[150]\ttrain_1-mae:0.062373\tvalid_1-mae:0.062367\n",
      "[160]\ttrain_1-mae:0.062005\tvalid_1-mae:0.06203\n",
      "[170]\ttrain_1-mae:0.061742\tvalid_1-mae:0.061791\n",
      "[180]\ttrain_1-mae:0.06156\tvalid_1-mae:0.061631\n",
      "[190]\ttrain_1-mae:0.061416\tvalid_1-mae:0.061524\n",
      "[200]\ttrain_1-mae:0.061297\tvalid_1-mae:0.061429\n",
      "[210]\ttrain_1-mae:0.061214\tvalid_1-mae:0.061375\n",
      "[220]\ttrain_1-mae:0.061141\tvalid_1-mae:0.061329\n",
      "[230]\ttrain_1-mae:0.061064\tvalid_1-mae:0.061281\n",
      "[240]\ttrain_1-mae:0.060997\tvalid_1-mae:0.061236\n",
      "[250]\ttrain_1-mae:0.060947\tvalid_1-mae:0.061216\n",
      "[260]\ttrain_1-mae:0.060904\tvalid_1-mae:0.061188\n",
      "[270]\ttrain_1-mae:0.060883\tvalid_1-mae:0.061199\n",
      "[280]\ttrain_1-mae:0.060852\tvalid_1-mae:0.06119\n",
      "[290]\ttrain_1-mae:0.060821\tvalid_1-mae:0.061179\n",
      "[300]\ttrain_1-mae:0.060794\tvalid_1-mae:0.061188\n",
      "[310]\ttrain_1-mae:0.060765\tvalid_1-mae:0.061178\n",
      "[320]\ttrain_1-mae:0.060728\tvalid_1-mae:0.061164\n",
      "[330]\ttrain_1-mae:0.060692\tvalid_1-mae:0.061141\n",
      "[340]\ttrain_1-mae:0.060666\tvalid_1-mae:0.061148\n",
      "[350]\ttrain_1-mae:0.060633\tvalid_1-mae:0.061153\n",
      "[360]\ttrain_1-mae:0.060612\tvalid_1-mae:0.061157\n",
      "[370]\ttrain_1-mae:0.060588\tvalid_1-mae:0.061152\n",
      "[380]\ttrain_1-mae:0.060575\tvalid_1-mae:0.061159\n",
      "[390]\ttrain_1-mae:0.060534\tvalid_1-mae:0.061142\n",
      "[400]\ttrain_1-mae:0.060515\tvalid_1-mae:0.061141\n",
      "[410]\ttrain_1-mae:0.060504\tvalid_1-mae:0.061153\n",
      "[420]\ttrain_1-mae:0.060476\tvalid_1-mae:0.061141\n",
      "[430]\ttrain_1-mae:0.060444\tvalid_1-mae:0.06113\n",
      "[440]\ttrain_1-mae:0.060431\tvalid_1-mae:0.061135\n",
      "[450]\ttrain_1-mae:0.060401\tvalid_1-mae:0.061128\n",
      "[460]\ttrain_1-mae:0.060361\tvalid_1-mae:0.061106\n",
      "[470]\ttrain_1-mae:0.060354\tvalid_1-mae:0.061118\n",
      "[480]\ttrain_1-mae:0.060333\tvalid_1-mae:0.061111\n",
      "[490]\ttrain_1-mae:0.060318\tvalid_1-mae:0.061117\n",
      "[500]\ttrain_1-mae:0.060303\tvalid_1-mae:0.061124\n",
      "[510]\ttrain_1-mae:0.060259\tvalid_1-mae:0.061106\n",
      "[520]\ttrain_1-mae:0.060258\tvalid_1-mae:0.061118\n",
      "[530]\ttrain_1-mae:0.060235\tvalid_1-mae:0.061115\n",
      "[540]\ttrain_1-mae:0.060221\tvalid_1-mae:0.061115\n",
      "[550]\ttrain_1-mae:0.060192\tvalid_1-mae:0.061099\n",
      "[560]\ttrain_1-mae:0.060179\tvalid_1-mae:0.061104\n",
      "[570]\ttrain_1-mae:0.060162\tvalid_1-mae:0.061107\n",
      "[580]\ttrain_1-mae:0.060142\tvalid_1-mae:0.061101\n",
      "[590]\ttrain_1-mae:0.060127\tvalid_1-mae:0.061101\n",
      "[600]\ttrain_1-mae:0.060112\tvalid_1-mae:0.061101\n",
      "[610]\ttrain_1-mae:0.060095\tvalid_1-mae:0.061104\n",
      "[620]\ttrain_1-mae:0.060071\tvalid_1-mae:0.0611\n",
      "[630]\ttrain_1-mae:0.060044\tvalid_1-mae:0.061089\n",
      "[640]\ttrain_1-mae:0.060016\tvalid_1-mae:0.061084\n",
      "[650]\ttrain_1-mae:0.059998\tvalid_1-mae:0.061086\n",
      "[660]\ttrain_1-mae:0.059991\tvalid_1-mae:0.061098\n",
      "[670]\ttrain_1-mae:0.059976\tvalid_1-mae:0.061098\n",
      "[680]\ttrain_1-mae:0.059948\tvalid_1-mae:0.061087\n",
      "[690]\ttrain_1-mae:0.059942\tvalid_1-mae:0.061102\n",
      "[700]\ttrain_1-mae:0.059923\tvalid_1-mae:0.061092\n",
      "[710]\ttrain_1-mae:0.059906\tvalid_1-mae:0.061098\n",
      "[720]\ttrain_1-mae:0.059885\tvalid_1-mae:0.061092\n",
      "[730]\ttrain_1-mae:0.059872\tvalid_1-mae:0.0611\n",
      "[740]\ttrain_1-mae:0.05984\tvalid_1-mae:0.061082\n",
      "[750]\ttrain_1-mae:0.059824\tvalid_1-mae:0.061075\n",
      "[760]\ttrain_1-mae:0.059808\tvalid_1-mae:0.061083\n",
      "[770]\ttrain_1-mae:0.059797\tvalid_1-mae:0.061089\n",
      "[780]\ttrain_1-mae:0.059785\tvalid_1-mae:0.061099\n",
      "[790]\ttrain_1-mae:0.059772\tvalid_1-mae:0.061099\n",
      "[800]\ttrain_1-mae:0.059759\tvalid_1-mae:0.061101\n",
      "[810]\ttrain_1-mae:0.059747\tvalid_1-mae:0.061105\n",
      "[820]\ttrain_1-mae:0.059724\tvalid_1-mae:0.061103\n",
      "[830]\ttrain_1-mae:0.059698\tvalid_1-mae:0.061089\n",
      "[840]\ttrain_1-mae:0.05969\tvalid_1-mae:0.061097\n",
      "[850]\ttrain_1-mae:0.059671\tvalid_1-mae:0.061094\n",
      "Stopping. Best iteration:\n",
      "[750]\ttrain_1-mae:0.059824\tvalid_1-mae:0.061075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Cross Validation ###\n",
    "\n",
    "# We are dividing our datasets into the training and validation sets so that we could monitor and the test the\n",
    "# progress of our machine learning algorithm. This would let us know when our model might be over or under fitting\n",
    "# on the dataset that we have employed.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = x_train\n",
    "y1 = y_train_1 \n",
    "y2 = y_train_2\n",
    "\n",
    "Xtrain_1, Xvalid_1, ytrain_1, yvalid_1 = train_test_split(X, y1, test_size=0.2, random_state=42)\n",
    "Xtrain_2, Xvalid_2, ytrain_2, yvalid_2 = train_test_split(X, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "assert((Xtrain_1 == Xtrain_2).all())\n",
    "\n",
    "# Implement the Xgboost #\n",
    "\n",
    "# We can now select the parameters for Xgboost and monitor the progress of results on our validation set. The\n",
    "# explanation of the xgboost parameters and what they do can be found on the following link\n",
    "# http://xgboost.readthedocs.io/en/latest/parameter.html \n",
    "\n",
    "dtrain_1 = xgb.DMatrix(Xtrain_1, label=ytrain_1)\n",
    "dvalid_1 = xgb.DMatrix(Xvalid_1, label=yvalid_1)\n",
    "\n",
    "# Try different parameters! \n",
    "xgb_params = {'min_child_weight': 5, 'eta': 0.035, 'colsample_bytree': 0.5, 'max_depth': 4,\n",
    "            'subsample': 0.85, 'lambda': 0.8, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1, 'gamma' : 0,\n",
    "            'eval_metric': 'mae', 'objective': 'reg:linear' }           \n",
    "\n",
    "watchlist = [(dtrain_1, 'train_1'), (dvalid_1, 'valid_1')]\n",
    "\n",
    "model_xgb_1 = xgb.train(xgb_params, dtrain_1, 1000, watchlist, early_stopping_rounds=100,\n",
    "                  maximize=False, verbose_eval=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y_train_2\n",
    "# print(y2)\n",
    "Xtrain_2, Xvalid_2, ytrain_2, yvalid_2 = train_test_split(X, y2, test_size=0.2, random_state=42)\n",
    "# print(ytrain_2)\n",
    "# print(yvalid_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain_2-merror:0.42259\tvalid_2-merror:0.431741\n",
      "Multiple eval metrics have been passed: 'valid_2-merror' will be used for early stopping.\n",
      "\n",
      "Will train until valid_2-merror hasn't improved in 100 rounds.\n",
      "[10]\ttrain_2-merror:0.398228\tvalid_2-merror:0.420335\n",
      "[20]\ttrain_2-merror:0.38292\tvalid_2-merror:0.416821\n",
      "[30]\ttrain_2-merror:0.372325\tvalid_2-merror:0.416016\n",
      "[40]\ttrain_2-merror:0.364381\tvalid_2-merror:0.415153\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-47b843948d08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m model_xgb_2 = xgb.train(xgb_params, dtrain_2, 1000, watchlist, early_stopping_rounds=100,\n\u001b[0;32m---> 24\u001b[0;31m                   maximize=False, verbose_eval=10)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 894\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Cross Validation ###\n",
    "\n",
    "# We are dividing our datasets into the training and validation sets so that we could monitor and the test the\n",
    "# progress of our machine learning algorithm. This would let us know when our model might be over or under fitting\n",
    "# on the dataset that we have employed.\n",
    "\n",
    "# Implement the Xgboost #\n",
    "\n",
    "# We can now select the parameters for Xgboost and monitor the progress of results on our validation set. The\n",
    "# explanation of the xgboost parameters and what they do can be found on the following link\n",
    "# http://xgboost.readthedocs.io/en/latest/parameter.html \n",
    "\n",
    "dtrain_2 = xgb.DMatrix(Xtrain_2, label=ytrain_2)\n",
    "dvalid_2 = xgb.DMatrix(Xvalid_2, label=yvalid_2)\n",
    "\n",
    "# Try different parameters! \n",
    "xgb_params = {'min_child_weight': 5, 'eta': 0.3, 'colsample_bytree': 0.5, 'max_depth': 6,\n",
    "            'subsample': 0.85, 'lambda': 0.8, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1, 'gamma' : 0.1,\n",
    "            'eval_metric': 'merror', 'objective': 'multi:softmax', 'num_class': 2}           \n",
    "\n",
    "watchlist = [(dtrain_2, 'train_2'), (dvalid_2, 'valid_2')]\n",
    "\n",
    "model_xgb_2 = xgb.train(xgb_params, dtrain_2, 1000, watchlist, early_stopping_rounds=100,\n",
    "                  maximize=False, verbose_eval=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTING 2016 ...\n",
      "PREDICTING 2017 ...\n",
      "    parcelid  201610  201611  201612  201710  201711  201712\n",
      "0   10754147       1       1       1       1       1       1\n",
      "1   10759547      -1      -1      -1       1       1       1\n",
      "2   10843547       1       1       1       1       1       1\n",
      "3   10859147       1       1       1       1       1       1\n",
      "4   10879947       1       1       1       1       1       1\n",
      "5   10898347       1       1       1       1       1       1\n",
      "6   10933547       1       1       1       1       1       1\n",
      "7   10940747       1       1       1       1       1       1\n",
      "8   10954547      -1      -1      -1      -1      -1      -1\n",
      "9   10976347       1       1       1       1       1       1\n",
      "10  11073947       1       1       1       1       1       1\n",
      "11  11114347      -1      -1      -1      -1      -1      -1\n",
      "12  11116947      -1      -1      -1       1       1       1\n",
      "13  11142747       1       1       1      -1      -1      -1\n",
      "14  11193347      -1      -1      -1      -1      -1      -1\n",
      "15  11215747      -1      -1      -1      -1      -1      -1\n",
      "16  11229347      -1      -1      -1       1       1       1\n",
      "17  11287347       1       1       1       1       1       1\n",
      "18  11288547       1       1       1       1       1       1\n",
      "19  11324547       1       1       1       1       1       1\n",
      "20  11391347       1       1       1       1       1       1\n",
      "21  11395747       1       1       1       1       1       1\n",
      "22  11404347       1       1       1       1       1       1\n",
      "23  11405747      -1      -1      -1       1       1       1\n",
      "24  11417147       1       1       1       1       1       1\n",
      "25  11457547       1       1       1       1       1       1\n",
      "26  11488147       1       1       1       1       1       1\n",
      "27  11520747       1       1       1       1       1       1\n",
      "28  11524947       1       1       1       1       1       1\n",
      "29  11544747       1       1       1       1       1       1\n",
      "30  11579347       1       1       1       1       1       1\n",
      "31  11585547       1       1       1      -1      -1      -1\n",
      "32  11588747      -1      -1      -1      -1      -1      -1\n",
      "33  11592147       1       1       1       1       1       1\n",
      "34  11609547       1       1       1       1       1       1\n",
      "35  11617547       1       1       1       1       1       1\n",
      "36  11660547      -1      -1      -1       1       1       1\n",
      "37  11681747       1       1       1       1       1       1\n",
      "38  11698747       1       1       1       1       1       1\n",
      "39  11704347       1       1       1       1       1       1\n",
      "40  11732547       1       1       1       1       1       1\n",
      "41  11742147       1       1       1       1       1       1\n",
      "42  11751547       1       1       1       1       1       1\n",
      "43  11759947      -1      -1      -1       1       1       1\n",
      "44  11780147       1       1       1       1       1       1\n",
      "45  11780347       1       1       1       1       1       1\n",
      "46  11781947       1       1       1      -1      -1      -1\n",
      "47  11808747       1       1       1       1       1       1\n",
      "48  11817747       1       1       1       1       1       1\n",
      "49  11818747      -1      -1      -1      -1      -1      -1\n"
     ]
    }
   ],
   "source": [
    "# Predicting the results #\n",
    "\n",
    "# Let us now predict the target variable for our test dataset. All we have to do now is just fit the already\n",
    "# trained model on the test set that we had made merging the sample file with properties dataset #\n",
    "\n",
    "months = [10, 11, 12]\n",
    "years = [2016, 2017]\n",
    "\n",
    "\n",
    "for y in years:\n",
    "    if y == 2016:\n",
    "        x_test = df_test_2016\n",
    "    else:\n",
    "        x_test = df_test_2017\n",
    "\n",
    "    print(\"PREDICTING \" + str(y) + \" ...\")\n",
    "    \n",
    "    dtest = xgb.DMatrix(x_test.values)\n",
    "\n",
    "    Predicted_test_xgb = model_xgb_2.predict(dtest)\n",
    "\n",
    "    \n",
    "    for m in months:        \n",
    "        name = str(y) + str(m)\n",
    "        output[name] = Predicted_test_xgb\n",
    "        output[name] = output.apply(lambda row: -1 if row[name] < 0.5 else 1, axis=1)\n",
    "\n",
    "print(output.head(50))\n",
    "# Submitting the Results \n",
    "# print('Preparing the csv file ...')\n",
    "# output.to_csv(DirBase+'xgb_predicted_results_mod8.csv', index=False, float_format='%.4f')\n",
    "# print(\"Finished writing the file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTING 2016 ...\n",
      "PREDICTING 2017 ...\n",
      "    parcelid    201610    201611    201612    201710    201711    201712\n",
      "0   10754147  0.253454  0.253454  0.253454  0.226232  0.226232  0.226232\n",
      "1   10759547 -0.092452 -0.092452 -0.092452  0.135281  0.135281  0.135281\n",
      "2   10843547  0.427953  0.427953  0.427953  0.612129  0.612129  0.612129\n",
      "3   10859147  0.536065  0.536065  0.536065  0.512513  0.512513  0.512513\n",
      "4   10879947  0.391063  0.391063  0.391063  0.425663  0.425663  0.425663\n",
      "5   10898347  0.439333  0.439333  0.439333  0.431588  0.431588  0.431588\n",
      "6   10933547  0.118800  0.118800  0.118800  0.121055  0.121055  0.121055\n",
      "7   10940747  0.274317  0.274317  0.274317  0.268063  0.268063  0.268063\n",
      "8   10954547 -0.306277 -0.306277 -0.306277 -0.261130 -0.261130 -0.261130\n",
      "9   10976347  0.303563  0.303563  0.303563  0.296585  0.296585  0.296585\n",
      "10  11073947  0.404283  0.404283  0.404283  0.443952  0.443952  0.443952\n",
      "11  11114347 -0.039725 -0.039725 -0.039725 -0.021680 -0.021680 -0.021680\n",
      "12  11116947 -0.076371 -0.076371 -0.076371  0.074705  0.074705  0.074705\n",
      "13  11142747  0.160528  0.160528  0.160528 -0.121156 -0.121156 -0.121156\n",
      "14  11193347 -0.077455 -0.077455 -0.077455 -0.073084 -0.073084 -0.073084\n",
      "15  11215747 -0.094903 -0.094903 -0.094903 -0.083531 -0.083531 -0.083531\n",
      "16  11229347 -0.349031 -0.349031 -0.349031  0.342577  0.342577  0.342577\n",
      "17  11287347  0.381977  0.381977  0.381977  0.380366  0.380366  0.380366\n",
      "18  11288547  0.347335  0.347335  0.347335  0.291099  0.291099  0.291099\n",
      "19  11324547  0.040669  0.040669  0.040669  0.041599  0.041599  0.041599\n",
      "20  11391347  0.192569  0.192569  0.192569  0.185574  0.185574  0.185574\n",
      "21  11395747  0.184090  0.184090  0.184090  0.172376  0.172376  0.172376\n",
      "22  11404347  0.313228  0.313228  0.313228  0.365223  0.365223  0.365223\n",
      "23  11405747 -0.259920 -0.259920 -0.259920  0.256528  0.256528  0.256528\n",
      "24  11417147  0.223806  0.223806  0.223806  0.222037  0.222037  0.222037\n",
      "25  11457547  0.465486  0.465486  0.465486  0.420754  0.420754  0.420754\n",
      "26  11488147  0.093631  0.093631  0.093631  0.093393  0.093393  0.093393\n",
      "27  11520747  0.489616  0.489616  0.489616  0.486802  0.486802  0.486802\n",
      "28  11524947  0.059561  0.059561  0.059561  0.061508  0.061508  0.061508\n",
      "29  11544747  0.060415  0.060415  0.060415  0.057299  0.057299  0.057299\n",
      "30  11579347  0.411418  0.411418  0.411418  0.418545  0.418545  0.418545\n",
      "31  11585547  0.086672  0.086672  0.086672 -0.087693 -0.087693 -0.087693\n",
      "32  11588747 -0.117931 -0.117931 -0.117931 -0.118574 -0.118574 -0.118574\n",
      "33  11592147  0.839970  0.839970  0.839970  0.813365  0.813365  0.813365\n",
      "34  11609547  0.466256  0.466256  0.466256  0.461494  0.461494  0.461494\n",
      "35  11617547  0.073159  0.073159  0.073159  0.083773  0.083773  0.083773\n",
      "36  11660547 -0.220049 -0.220049 -0.220049  0.201349  0.201349  0.201349\n",
      "37  11681747  0.121351  0.121351  0.121351  0.114027  0.114027  0.114027\n",
      "38  11698747  0.512647  0.512647  0.512647  0.502562  0.502562  0.502562\n",
      "39  11704347  0.289443  0.289443  0.289443  0.262252  0.262252  0.262252\n",
      "40  11732547  0.371180  0.371180  0.371180  0.368368  0.368368  0.368368\n",
      "41  11742147  0.358380  0.358380  0.358380  0.355929  0.355929  0.355929\n",
      "42  11751547  0.492572  0.492572  0.492572  0.493074  0.493074  0.493074\n",
      "43  11759947 -0.169235 -0.169235 -0.169235  0.146006  0.146006  0.146006\n",
      "44  11780147  0.102409  0.102409  0.102409  0.054702  0.054702  0.054702\n",
      "45  11780347  0.139507  0.139507  0.139507  0.402470  0.402470  0.402470\n",
      "46  11781947  0.478440  0.478440  0.478440 -0.579785 -0.579785 -0.579785\n",
      "47  11808747  0.226630  0.226630  0.226630  0.216749  0.216749  0.216749\n",
      "48  11817747  0.481701  0.481701  0.481701  0.473621  0.473621  0.473621\n",
      "49  11818747 -0.118676 -0.118676 -0.118676 -0.138586 -0.138586 -0.138586\n"
     ]
    }
   ],
   "source": [
    "# Predicting the results #\n",
    "\n",
    "# Let us now predict the target variable for our test dataset. All we have to do now is just fit the already\n",
    "# trained model on the test set that we had made merging the sample file with properties dataset #\n",
    "\n",
    "months = [10, 11, 12]\n",
    "years = [2016, 2017]\n",
    "\n",
    "\n",
    "for y in years:\n",
    "    if y == 2016:\n",
    "        x_test = df_test_2016\n",
    "    else:\n",
    "        x_test = df_test_2017\n",
    "\n",
    "    print(\"PREDICTING \" + str(y) + \" ...\")\n",
    "    \n",
    "    dtest = xgb.DMatrix(x_test.values)\n",
    "\n",
    "    Predicted_test_xgb = model_xgb_1.predict(dtest)\n",
    "\n",
    "    \n",
    "    for m in months:        \n",
    "        name = str(y) + str(m)\n",
    "        output[name] = output[name] * Predicted_test_xgb\n",
    "\n",
    "print(output.head(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the csv file ...\n",
      "Finished writing the file\n"
     ]
    }
   ],
   "source": [
    "# Submitting the Results \n",
    "print('Preparing the csv file ...')\n",
    "output.to_csv(DirBase+'xgb_predicted_results_mod12.csv', index=False, float_format='%.4f')\n",
    "print(\"Finished writing the file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
